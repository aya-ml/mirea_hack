{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from docx import Document as Doc\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Эмбеддинги текста из документа:\")\\nprint(cls_embedding)\\n\\nprint(\"Структура документа сохранена в \\'document_structure.json\\'\")\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Путь к файлу\n",
    "infile = 'labar3new.docx'\n",
    "\n",
    "# Открываем документ\n",
    "doc = Doc(infile)\n",
    "\n",
    "# Инициализация токенизатора и модели BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Сбор текста, таблиц и списков\n",
    "all_text = []\n",
    "structured_tables = []  # Для таблиц\n",
    "structured_lists = []   # Для списков\n",
    "list_checker = True     # Если есть пробел делаем False\n",
    "\n",
    "def is_list_paragraph(paragraph):\n",
    "    \"\"\"\n",
    "    Проверяет, является ли параграф частью списка (на основе стилей).\n",
    "    \"\"\"\n",
    "    style_name = paragraph.style.name.lower()\n",
    "    return \"list\" in style_name or \"bullet\" in style_name or \"number\" in style_name\n",
    "\n",
    "current_list = []  # Временное хранилище для текущего списка\n",
    "\n",
    "for paragraph in doc.paragraphs:\n",
    "    text = paragraph.text.strip()\n",
    "    \n",
    "    # Проверяем, является ли параграф списком\n",
    "    if is_list_paragraph(paragraph):\n",
    "        if text:  # Добавляем элемент в текущий список\n",
    "            current_list.append(text)\n",
    "    else:\n",
    "        # Если список закончился, сохраняем его\n",
    "        if current_list:\n",
    "            structured_lists.append(current_list)\n",
    "            all_text.extend(current_list)  # Добавляем текст списка в общий массив\n",
    "            current_list = []\n",
    "        # Обрабатываем обычный параграф\n",
    "        if text:\n",
    "            all_text.append(text)\n",
    "\n",
    "# Добавляем последний список, если он остался\n",
    "if current_list:\n",
    "    structured_lists.append(current_list)\n",
    "    all_text.extend(current_list)\n",
    "\n",
    "# Обработка таблиц\n",
    "for table in doc.tables:\n",
    "    table_data = []\n",
    "    for row in table.rows:\n",
    "        row_data = [cell.text.strip() for cell in row.cells]\n",
    "        table_data.append(row_data)\n",
    "    structured_tables.append({\"data\": table_data})\n",
    "\n",
    "    # Конвертируем строки таблиц в текст\n",
    "    table_text = [\" | \".join(row) for row in table_data]\n",
    "    all_text.extend(table_text)\n",
    "\n",
    "# Объединяем текст для токенизации\n",
    "combined_text = \"\\n\".join(all_text)\n",
    "\n",
    "# Токенизация текста\n",
    "inputs = tokenizer(combined_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Получение эмбеддингов из BERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Вывод скрытых состояний [CLS]-токена\n",
    "cls_embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "# Сохранение списков и таблиц в JSON\n",
    "output_data = {\n",
    "    \"lists\": structured_lists,\n",
    "    \"tables\": structured_tables\n",
    "}\n",
    "\n",
    "with open(\"document_structure.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "'''\n",
    "print(\"Эмбеддинги текста из документа:\")\n",
    "print(cls_embedding)\n",
    "\n",
    "print(\"Структура документа сохранена в 'document_structure.json'\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'data': [[''], ['МИНОБРНАУКИ РОССИИ'], ['Федеральное государственное бюджетное образовательное учреждение\\nвысшего образования\\n«МИРЭА – Российский технологический университет»\\nРТУ МИРЭА']]}, {'data': [['№', '1', '2'], ['', '1', '1'], ['', '1', '3'], ['(абс)', '0.008495', '0.0009265'], ['(отн) %', '0.8495', '0.03088']]}, {'data': [['№', '1', '2'], ['', '0', '2'], ['', '1', '1'], ['(абс)', '-0.00003889', '-0.5724'], ['(отн) %', '-0.003889 %', '-57.24 %']]}, {'data': [['№', '1', '2'], ['', '0', '0'], ['', '0.3*sin(2*t)+2', '0.5*sin(2*t)+2']]}, {'data': [['№', '1', '2', '3', '4'], ['', '0', '0', '0', '0'], ['', '1', '1', '1', '1'], ['dt', '0.0001', '0.001', '0.005', '0.01'], ['(абс)', '-0.00003889', '0.001029', '19.4', '1096'], ['(отн) %', '-0.003889', '0.1029', '1940', '109600']]}]\n"
     ]
    }
   ],
   "source": [
    "print(structured_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.47s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кто автор работы?\n",
      "Автор работы - студент группы КРБО-01-19 Ткачев Н. А.\n"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "references = [combined_text]\n",
    "\n",
    "documents = [Document(\n",
    "    page_content=ref,\n",
    "    metadata={\"source\": \"tweet\"}\n",
    "    )\n",
    "    for ref in references]\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "db = FAISS.from_documents(documents, OllamaEmbeddings(model=\"nomic-embed-text\",show_progress=True))\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "# Set up the local model:\n",
    "local_model = \"llama3\"\n",
    "llm = ChatOllama(model=local_model, num_predict=400,\n",
    "                 stop=[\"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"])\n",
    "\n",
    "# Set up the RAG chain:\n",
    "prompt_template = \"\"\"\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Responds user questions taking into accoun the given context, give a precise and short answer without referring to this part of prompt.\n",
    "Question: {question}\n",
    "Context: {context}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Querying the LLM (oviously to test here you must ask a relevant question of your data)\n",
    "question = \"Кто автор работы?\"\n",
    "candidate = [rag_chain.invoke(question)]\n",
    "print(question)\n",
    "print(candidate[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.faiss.FAISS object at 0x000002808E87FB80>\n"
     ]
    }
   ],
   "source": [
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision: 0.8156, Recall: 0.5690, F1: 0.6703\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from bert_score import BERTScorer\n",
    "# Example texts\n",
    "reference = [combined_text]\n",
    "candidate = [rag_chain.invoke(question)]\n",
    "# BERTScore calculation\n",
    "scorer = BERTScorer(model_type='bert-base-uncased')\n",
    "P, R, F1 = scorer.score(candidate, reference)\n",
    "print(f\"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.82, Recall: 0.57, F1: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Define the reference and candidate sentences\n",
    "#reference = [\"The cat sat on the mat.\"]\n",
    "#candidate = [\"The cat is on the mat!\"]\n",
    "\n",
    "# Compute the BERTScore\n",
    "P, R, F1 = scorer.score(candidate, reference)\n",
    "\n",
    "# Print the scores\n",
    "print(\"Precision: {:.2f}, Recall: {:.2f}, F1: {:.2f}\".format(P.item(), R.item(), F1.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
